{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [    
    "## Table of Contents\n",
    "    \n",
    "1. [Exercise in Pre-trained models](#Exercise 1 in Pre-trained models (Lesson 2)\n",
    "    1. [Exercise 1.1](#1.1 Download 3 pre-trained models here)\n",
    "    2. [Exercise 1.2](#1.2 Preprocess the inputs to match what each of the models expects as their input.)\n",
    "    3. [Exercise 1.3](#1.3)\n",
    "2. [Exercise in Model Optimizer](#2)\n",
    "    1. [Exercise 2.1](#2.1)\n",
    "    2. [Exercise 2.2](#2.2)\n",
    "    3. [Exercise 2.3](#2.3)\n",
    "3. [Exercise in Inference Engine](#3)\n",
    "    1. [Exercise 3.1](#3.1)\n",
    "    2. [Exercise 3.2](#3.2)\n",
    "    3. [Exercise 3.3](#3.3)\n",
    "4. [Exercise in Deploy an Edge App](#4)\n",
    "    1. [Exercise 4.1](#4.1)\n",
    "    2. [Exercise 4.2](#4.2)\n",
    "    3. [Exercise 4.3](#4.3)\n"   
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 in Pre-trained models (Lesson 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.1 Download 3 pre-trained models [here](https://software.intel.com/en-us/openvino-toolkit/documentation/pretrained-models) to complete tasks including: *human pose estimation (all precision levels), text detection (FP16), determining car type and color (INT8)*. And verify the downloads afterwards "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "# to access model downloader in Linux\n",
    "cd /opt/intel/openvino/deployment_tools/open_model_zoo/tools/downloader\n",
    "# to download the three models, default is to download all precision levels\n",
    "sudo ./downloader.py --name human-pose-estimation-0001 -o {directory output where you wanna save the downloaded model}\n",
    "sudo ./downloader.py --name text-detection-0004 --precisions FP16 -o {dir output}\n",
    "sudo ./downloader.py --name vehicle-attributes-recognition-barrier-0039 --precisions INT8 -o {dir output}\n",
    "# when working on the local computer, save to the same dir as model downloader, it will create a new dir named /intel, two files .bin and .xml should be in each model folder\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess the inputs to match what each of the models expects as their input. \n",
    "- **<font size='3'>[human pose estimation](https://docs.openvinotoolkit.org/latest/_models_intel_human_pose_estimation_0001_description_human_pose_estimation_0001.html) </font>**\n",
    "- **<font size='3'>[text detection](http://docs.openvinotoolkit.org/latest/_models_intel_text_detection_0004_description_text_detection_0004.html) </font>**\n",
    "- **<font size='3'>[vehicle attributes recognition](https://docs.openvinotoolkit.org/latest/_models_intel_vehicle_attributes_recognition_barrier_0039_description_vehicle_attributes_recognition_barrier_0039.html) </font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def pose_estimation(input_image):\n",
    "    '''\n",
    "    Given some input image, preprocess the image so that\n",
    "    it can be used with the related pose estimation model\n",
    "    you downloaded previously. You can use cv2.resize()\n",
    "    to resize the image.\n",
    "    '''\n",
    "    preprocessed_image = np.copy(input_image)\n",
    "\n",
    "    # TODO: Preprocess the image for the pose estimation model\n",
    "    # default opencv color channel is BGR with H*W*C order\n",
    "    # cv2.resize input format is W*H and output format is H*W*C\n",
    "    preprocessed_image = cv2.resize(preprocessed_image,(456,256))\n",
    "    # move color channels from last dimension to first dimension, so the order is C*H*W\n",
    "    preprocessed_image = preprocessed_image.transpose(2,0,1)\n",
    "    # add an extra dimension 1 as batch size using reshape, and input format is 1*C*H*W\n",
    "    preprocessed_image = preprocessed_image.reshape(1,3,256,456)\n",
    "    \n",
    "    return preprocessed_image\n",
    "\n",
    "\n",
    "def text_detection(input_image):\n",
    "    '''\n",
    "    Given some input image, preprocess the image so that\n",
    "    it can be used with the related text detection model\n",
    "    you downloaded previously. You can use cv2.resize()\n",
    "    to resize the image.\n",
    "    '''\n",
    "    preprocessed_image = np.copy(input_image)\n",
    "\n",
    "    # TODO: Preprocess the image for the text detection model\n",
    "    preprocessed_image = cv2.resize(preprocessed_image,(1280,768))\n",
    "    preprocessed_image = preprocessed_image.transpose(2,0,1)\n",
    "    preprocessed_image = preprocessed_image.reshape(1,3,768,1280)\n",
    "    \n",
    "    return preprocessed_image\n",
    "\n",
    "\n",
    "def car_meta(input_image):\n",
    "    '''\n",
    "    Given some input image, preprocess the image so that\n",
    "    it can be used with the related car metadata model\n",
    "    you downloaded previously. You can use cv2.resize()\n",
    "    to resize the image.\n",
    "    '''\n",
    "    preprocessed_image = np.copy(input_image)\n",
    "\n",
    "    # TODO: Preprocess the image for the car metadata model\n",
    "    preprocessed_image = cv2.resize(preprocessed_image,(72,72))\n",
    "    preprocessed_image = preprocessed_image.transpose(2,0,1)\n",
    "    preprocessed_image = preprocessed_image.reshape(1,3,72,72)\n",
    "    \n",
    "    return preprocessed_image\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='1.3'></a>1.3 Call functions to handle input and output of models within the app for pose detection, car-attributes detection, and text detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size='3'>First, revise some codes in ```app.py``` and ```handle_model.py``` to print out the keys (```print(output.keys())```) of the model output (take pose detection for example). By running the python app.py and specifying the location of the image (```-i```), the model file (```-m```), the type of model (```-t```), and CPU extension file (```-c```), terminal will print out two keys, ```'Mconv7_stage2_L2'``` and ```'Mconv7_stage2_L1'```. Then use either key and go back to ```handle_model.py``` to print out the shape of the corresponding value, and compare to the output in the document, so I can know which key I should use.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **<font size='3'>handle_pose function</font>**\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def handle_pose(output, input_shape):\n",
    "    '''\n",
    "    Handles the output of the Pose Estimation model.\n",
    "    Returns ONLY the keypoint heatmaps, and not the Part Affinity Fields.\n",
    "    '''\n",
    "    # TODO 1: Extract only the second blob output (keypoint heatmaps)\n",
    "    # the second output blob contains heatmap keypoints with shape of 1*19*32*57\n",
    "    # heatmaps[0]: 19*32*57\n",
    "    heatmaps = output['Mconv7_stage2_L2']\n",
    "    \n",
    "    # TODO 2: Resize the heatmap back to the size of the input\n",
    "    # input_shape: H*W*C\n",
    "    # input_shape[0:2][::-1]: W*H, this order is required by cv2.resize function, output is in H*W order\n",
    "    pose_output = np.zeros([heatmaps.shape[1],input_shape[0],input_shape[1]])\n",
    "    # len(heatmaps[0]): 19\n",
    "    for i in range(len(heatmaps[0])):\n",
    "        pose_output[i] = cv2.resize(heatmaps[0][i],input_shape[0:2][::-1])\n",
    "\n",
    "    return pose_output\n",
    "\n",
    "\n",
    "def handle_text(output, input_shape):\n",
    "    '''\n",
    "    Handles the output of the Text Detection model.\n",
    "    Returns ONLY the text/no text classification of each pixel,\n",
    "        and not the linkage between pixels and their neighbors.\n",
    "    '''\n",
    "    # TODO 1: Extract only the first blob output (text/no text classification)\n",
    "    txt_bin = output['model/segm_logits/add']\n",
    "    # TODO 2: Resize this output back to the size of the input\n",
    "    txt_output = np.empty([txt_bin.shape[1],input_shape[0],input_shape[1]])\n",
    "    for i in range(len(txt_bin[0])):\n",
    "        txt_output[i] = cv2.resize(txt_bin[0][i],input_shape[0:2][::-1])\n",
    "\n",
    "    return txt_output\n",
    "\n",
    "\n",
    "def handle_car(output, input_shape):\n",
    "    '''\n",
    "    Handles the output of the Car Metadata model.\n",
    "    Returns two integers: the argmax of each softmax output.\n",
    "    The first is for color, and the second for type.\n",
    "    '''\n",
    "    # TODO 1: Get the argmax of the \"color\" output\n",
    "    color_arg = output['color'].flatten().argmax()\n",
    "    # TODO 2: Get the argmax of the \"type\" output\n",
    "    type_arg = output['type'].flatten().argmax()\n",
    "    return color_arg,type_arg\n",
    "\n",
    "\n",
    "def handle_output(model_type):\n",
    "    '''\n",
    "    Returns the related function to handle an output,\n",
    "        based on the model_type being used.\n",
    "    '''\n",
    "    if model_type == \"POSE\":\n",
    "        return handle_pose\n",
    "    elif model_type == \"TEXT\":\n",
    "        return handle_text\n",
    "    elif model_type == \"CAR_META\":\n",
    "        return handle_car\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "'''\n",
    "The below function is carried over from the previous exercise.\n",
    "You just need to call it appropriately in `app.py` to preprocess\n",
    "the input image.\n",
    "'''\n",
    "def preprocessing(input_image, height, width):\n",
    "    '''\n",
    "    Given an input image, height and width:\n",
    "    - Resize to width and height\n",
    "    - Transpose the final \"channel\" dimension to be first\n",
    "    - Reshape the image to add a \"batch\" of 1 at the start \n",
    "    '''\n",
    "    image = np.copy(input_image)\n",
    "    image = cv2.resize(image, (width, height))\n",
    "    image = image.transpose((2,0,1))\n",
    "    image = image.reshape(1, 3, height, width)\n",
    "\n",
    "    return image\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **<font size='3'>app.py</font>**\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from handle_models import handle_output, preprocessing\n",
    "from inference import Network\n",
    "\n",
    "\n",
    "CAR_COLORS = [\"white\", \"gray\", \"yellow\", \"red\", \"green\", \"blue\", \"black\"]\n",
    "CAR_TYPES = [\"car\", \"bus\", \"truck\", \"van\"]\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    '''\n",
    "    Gets the arguments from the command line.\n",
    "    '''\n",
    "\n",
    "    parser = argparse.ArgumentParser(\"Basic Edge App with Inference Engine\")\n",
    "    # -- Create the descriptions for the commands\n",
    "\n",
    "    c_desc = \"CPU extension file location, if applicable\"\n",
    "    d_desc = \"Device, if not CPU (GPU, FPGA, MYRIAD)\"\n",
    "    i_desc = \"The location of the input image\"\n",
    "    m_desc = \"The location of the model XML file\"\n",
    "    t_desc = \"The type of model: POSE, TEXT or CAR_META\"\n",
    "\n",
    "    # -- Add required and optional groups\n",
    "    parser._action_groups.pop()\n",
    "    required = parser.add_argument_group('required arguments')\n",
    "    optional = parser.add_argument_group('optional arguments')\n",
    "\n",
    "    # -- Create the arguments\n",
    "    required.add_argument(\"-i\", help=i_desc, required=True)\n",
    "    required.add_argument(\"-m\", help=m_desc, required=True)\n",
    "    required.add_argument(\"-t\", help=t_desc, required=True)\n",
    "    optional.add_argument(\"-c\", help=c_desc, default=None)\n",
    "    optional.add_argument(\"-d\", help=d_desc, default=\"CPU\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_mask(processed_output):\n",
    "    '''\n",
    "    Given an input image size and processed output for a semantic mask,\n",
    "    returns a masks able to be combined with the original image.\n",
    "    '''\n",
    "    # Create an empty array for other color channels of mask\n",
    "    empty = np.zeros(processed_output.shape)\n",
    "    # Stack to make a Green mask where text detected\n",
    "    mask = np.dstack((empty, processed_output, empty))\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_output_image(model_type, image, output):\n",
    "    '''\n",
    "    Using the model type, input image, and processed output,\n",
    "    creates an output image showing the result of inference.\n",
    "    '''\n",
    "    if model_type == \"POSE\":\n",
    "        # Remove final part of output not used for heatmaps\n",
    "        output = output[:-1]\n",
    "        # Get only pose detections above 0.5 confidence, set to 255\n",
    "        for c in range(len(output)):\n",
    "            output[c] = np.where(output[c]>0.5, 255, 0)\n",
    "        # Sum along the \"class\" axis\n",
    "        output = np.sum(output, axis=0)\n",
    "        # Get semantic mask\n",
    "        pose_mask = get_mask(output)\n",
    "        # Combine with original image\n",
    "        image = image + pose_mask\n",
    "        return image\n",
    "    elif model_type == \"TEXT\":\n",
    "        # Get only text detections above 0.5 confidence, set to 255\n",
    "        output = np.where(output[1]>0.5, 255, 0)\n",
    "        # Get semantic mask\n",
    "        text_mask = get_mask(output)\n",
    "        # Add the mask to the image\n",
    "        image = image + text_mask\n",
    "        return image\n",
    "#         print(output['model/link_logits_/add'].shape)\n",
    "    elif model_type == \"CAR_META\":\n",
    "        # Get the color and car type from their lists\n",
    "        color = CAR_COLORS[output[0]]\n",
    "        car_type = CAR_TYPES[output[1]]\n",
    "        # Scale the output text by the image shape\n",
    "        scaler = max(int(image.shape[0] / 1000), 1)\n",
    "        # Write the text of color and type onto the image\n",
    "        image = cv2.putText(image, \n",
    "            \"Color: {}, Type: {}\".format(color, car_type), \n",
    "            (50 * scaler, 100 * scaler), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "            2 * scaler, (255, 255, 255), 3 * scaler)\n",
    "        return image\n",
    "    else:\n",
    "        print(\"Unknown model type, unable to create output image.\")\n",
    "        return image\n",
    "\n",
    "\n",
    "def perform_inference(args):\n",
    "    '''\n",
    "    Performs inference on an input image, given a model.\n",
    "    '''\n",
    "    # Create a Network for using the Inference Engine\n",
    "    inference_network = Network()\n",
    "    # Load the model in the network, and obtain its input shape\n",
    "    n, c, h, w = inference_network.load_model(args.m, args.d, args.c)\n",
    "\n",
    "    # Read the input image\n",
    "    image = cv2.imread(args.i)\n",
    "\n",
    "    ### TODO: Preprocess the input image\n",
    "    preprocessed_image = preprocessing(image,h,w)\n",
    "\n",
    "    # Perform synchronous inference on the image\n",
    "    inference_network.sync_inference(preprocessed_image)\n",
    "\n",
    "    # Obtain the output of the inference request\n",
    "    output = inference_network.extract_output()\n",
    "\n",
    "    ### TODO: Handle the output of the network, based on args.t\n",
    "    ### Note: This will require using `handle_output` to get the correct\n",
    "    ###       function, and then feeding the output to that function.\n",
    "    processed_output = handle_output(args.t)(output,image.shape)\n",
    "\n",
    "    # Create an output image based on network\n",
    "    output_image = create_output_image(args.t, image, processed_output)\n",
    "\n",
    "    # Save down the resulting image\n",
    "    cv2.imwrite(\"outputs/{}-output.png\".format(args.t), output_image)\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    perform_inference(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **<font size='3'>call for the app</font>**\n",
    "```shell\n",
    "python {path of app.py file} -m {path of model .xml file} -i {path of input image file} -t {type of model: 'POSE'/'TEXT'/'CAR_META'} -c {CPU extension file, workspace: '/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so'}\n",
    "# output would be saved in the outputt directory\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **<font size='3'>After running the ```app.py```, output image shown as below, from left to right are: pose detection, text detection, car attribute detection</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://r953259c958904xjupyterlfkz3ibsg.udacity-student-workspaces.com/files/outputs/POSE-output.png?_xsrf=2%7C75ac3ac5%7C03fdd4b1ed1bc0978e7d959920fcd572%7C1583120289' width='300' style='float: left;' title='pose'>\n",
    "<img src='https://r953259c958904xjupyterlfkz3ibsg.udacity-student-workspaces.com/files/outputs/TEXT-output.png?_xsrf=2%7C75ac3ac5%7C03fdd4b1ed1bc0978e7d959920fcd572%7C1583120289' width='300' style='float: left;'>\n",
    "<img src='https://r953259c958904xjupyterlfkz3ibsg.udacity-student-workspaces.com/files/outputs/CAR_META-output.png?_xsrf=2%7C75ac3ac5%7C03fdd4b1ed1bc0978e7d959920fcd572%7C1583120289' width='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 in Model Optimizer (Lesson 3) <a id='2'></a>\n",
    "    \n",
    "**Back to [TOC](#toc)**    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='2.1'></a>2.1 Convert a TensorFlow Model \n",
    "* **feed in the downloaded SSD MobileNet V2 COCO model's .pb file, documentation can be found [here](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html)**\n",
    "* **--reverse_input_channels documentation can be found [here](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Converting_Model_General.html#when_to_reverse_input_channels)**\n",
    "* **--reverse_input_channels --tensorflow_object_detection_api_pipeline_config documentations can be found [here](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_Object_Detection_API_Models.html)**\n",
    "\n",
    "```shell\n",
    "# to download file from link to current directory in Linux: wget <url>, if the path is too long can also do var=path then $var/file\n",
    "# to unzip a file: tar -xvf path_to_file\n",
    "# cd to the directory where model was downloaded\n",
    "# TensorFlow models are trained with images in RGB order, but Inference Engine load images input in BGR order, so need --reverse_input_channels    \n",
    "python <INSTALL_DIR>/deployment_tools/model_optimizer directory/mo_tf.py --input_model <INPUT_MODEL>.pb --reverse_input_channels --tensorflow_object_detection_api_pipeline_config <path_to_pipeline.config> --tensorflow_use_custom_operations_config <INSTALL_DIR>/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json\n",
    "```\n",
    "* **will generate two files: model_name.xml and model_name.bin in the same directory and show you the ellaps time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='2.2'></a>2.2 Convert a Caffe Model into IR using Model Optimizer\n",
    "* **clone a repository from github ```git clone <repository_url>```**\n",
    "* **need to specify ```--input_proto``` if the ```.prototxt``` file is not named the same as the model ```.caffemodel``` file. [convert a caffe model](https://docs.openvinotoolkit.org/2018_R5/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Caffe.html#caffe_specific_conversion_params)**\n",
    "* **If you notice poor performance in inference, you may need to specify mean ```--mean_values [R,G,B]``` and scale values ```-- scale num``` in your arguments. [specify the parameters](https://docs.openvinotoolkit.org/2018_R5/_docs_MO_DG_prepare_model_convert_model_Converting_Model_General.html)**\n",
    "\n",
    "```shell\n",
    "# cd to SqueezeNet/SqueezeNet_v1.1 first\n",
    "python /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_model squeezenet_v1.1.caffemodel --input_proto deploy.prototxt\n",
    "```\n",
    "* **will generate two files: model_name.xml and model_name.bin in the same directory and show you the execution time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='2.3'></a>2.3 Convert a ONNX Model into IR using Model Optimizer\n",
    "* **download file from link to current directory in Linux: ```wget <url>```, then unzip the file: ```tar -xvf path_to_file```**\n",
    "* **convert a onnx model [documentation](https://docs.openvinotoolkit.org/2018_R5/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html)**\n",
    "\n",
    "```shell\n",
    "# cd to bvlc_alexnet first\n",
    "python /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_model <model_name>.onnx\n",
    "```\n",
    "* **will generate two files: model_name.xml and model_name.bin in the same directory and show you the execution time.**\n",
    "* **PyTorch/Apple core models must be converted to ONNX format outside of OpenVINO before using Model Optimizer for conversion.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 in Inference Engine (IE) (Lesson 4) <a id='3'></a>\n",
    "    \n",
    "**Back to [TOC](#toc)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='3.1'></a>3.1 Feed an IR to IE\n",
    "* **import Python wrapper for IE ```from openvino.inference_engine import IECore, IENetwork``` ([IECore](https://docs.openvinotoolkit.org/2019_R3/classie__api_1_1IECore.html), [IENetwork](https://docs.openvinotoolkit.org/2019_R3/classie__api_1_1IENetwork.html))**\n",
    "* **add each IR as an ```IENetwork``` and check whether the layers of that network are supported by CPU ```ie.query_network(network,device_name)``` ([IENet Layer](https://docs.openvinotoolkit.org/2019_R3/classie__api_1_1IENetLayer.html))**\n",
    "* **since workspace is using intel CPU, add a CPU extension to ```IECore```: ```ie=IECore(), ie.add_extension(extension_path=CPU_EXTENSION, device_name=\"CPU\")```**\n",
    "* **after verify all layers are supported, load IR (model is ```.xml``` file, weights is ```.bin``` file) into IE to create an ```ExecutableNetwork```: ```ie.load_network(network, device_name)```**\n",
    "* **[Data Structures of IE API](https://docs.openvinotoolkit.org/2019_R3/ie_python_api.html)**\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "### TODO: Load the necessary libraries\n",
    "from openvino.inference_engine import IECore, IENetwork\n",
    "import os\n",
    "\n",
    "CPU_EXTENSION = \"/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so\"\n",
    "\n",
    "def get_args():\n",
    "    '''\n",
    "    Gets the arguments from the command line.\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\"Load an IR into the Inference Engine\")\n",
    "    # -- Create the descriptions for the commands\n",
    "    m_desc = \"The location of the model XML file\"\n",
    "\n",
    "    # -- Create the arguments\n",
    "    parser.add_argument(\"-m\", help=m_desc)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def load_to_IE(model_xml):\n",
    "    ### TODO: Load the Inference Engine API\n",
    "    plugin = IECore()\n",
    "    \n",
    "    ### TODO: Load IR files into their related class\n",
    "    model_bin = os.path.splitext(model_xml)[0] + '.bin'\n",
    "    net = IENetwork(model=model_xml, weights=model_bin)\n",
    "    \n",
    "    ### TODO: Add a CPU extension, if applicable. It's suggested to check\n",
    "    ###       your code for unsupported layers for practice before \n",
    "    ###       implementing this. Not all of the models may need it.\n",
    "    plugin.add_extension(extension_path=CPU_EXTENSION, device_name=\"CPU\")    \n",
    "    \n",
    "    ### TODO: Get the supported layers of the network\n",
    "    supported_layers = plugin.query_network(net,'CPU')\n",
    "    \n",
    "    ### TODO: Check for any unsupported layers, and let the user\n",
    "    ###       know if anything is missing. Exit the program, if so.\n",
    "    unsupported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n",
    "    \n",
    "    if len(unsupported_layers) != 0:\n",
    "        print('Unsupported layers found: %s' %unsupported_layers)\n",
    "        print('Check if extension available to be added')\n",
    "        exit(1)\n",
    "        \n",
    "    ### TODO: Load the network into the Inference Engine\n",
    "    exec_net = plugin.load_network(network=net, device_name=\"CPU\", num_requests=2)\n",
    "    print(\"IR successfully loaded into Inference Engine.\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    load_to_IE(args.m)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='3.2'></a>3.2 Send Inference Requests to IE\n",
    "* **```inference.py```: perform a synchronous and an asynchronous [inference requests](https://docs.openvinotoolkit.org/2019_R3/classie__api_1_1InferRequest.html) given an input image frame**\n",
    "* **```test.py```: to test ```inference.py```**\n",
    "* **synchronous request: wait and do nothing until inference completes, so only one frame processed once; asynchronous request: not hold everthing up if response is slow, can send frame for inference and simultaneously start preprocessing next frame while waiting**\n",
    "* **[```ExecutableNetwork```](https://docs.openvinotoolkit.org/2019_R3/classie__api_1_1ExecutableNetwork.html)**\n",
    "* **```exect_net.requests[request_id]```: a collection of inference requests, e.g., ```exect_net.requests[0]```: means the first request**\n",
    "\n",
    "**inference.py**\n",
    "```python\n",
    "import argparse\n",
    "import cv2\n",
    "from helpers import load_to_IE, preprocessing\n",
    "\n",
    "CPU_EXTENSION = \"/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so\"\n",
    "\n",
    "def get_args():\n",
    "    '''\n",
    "    Gets the arguments from the command line.\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\"Load an IR into the Inference Engine\")\n",
    "    # -- Create the descriptions for the commands\n",
    "    m_desc = \"The location of the model XML file\"\n",
    "    i_desc = \"The location of the image input\"\n",
    "    r_desc = \"The type of inference request: Async ('A') or Sync ('S')\"\n",
    "\n",
    "    # -- Create the arguments\n",
    "    parser.add_argument(\"-m\", help=m_desc)\n",
    "    parser.add_argument(\"-i\", help=i_desc)\n",
    "    parser.add_argument(\"-r\", help=i_desc)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def async_inference(exec_net, input_blob, image):\n",
    "    ### TODO: Add code to perform asynchronous inference\n",
    "    ### Note: Return the exec_net\n",
    "    infer_request = exec_net.start_async(request_id=0,inputs={input_blob:image})     \n",
    "    while True:\n",
    "        # -1: Waits until inference result becomes available (default value)\n",
    "        status = infer_request.wait(-1)\n",
    "        # status 0 means the execution has been completed for the request\n",
    "        if status == 0:\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "    return exec_net\n",
    "\n",
    "\n",
    "def sync_inference(exec_net, input_blob, image):\n",
    "    ### TODO: Add code to perform synchronous inference\n",
    "    ### Note: Return the result of inference\n",
    "    res = exec_net.infer(inputs={input_blob:image})\n",
    "    return res\n",
    "\n",
    "\n",
    "def perform_inference(exec_net, request_type, input_image, input_shape):\n",
    "    '''\n",
    "    Performs inference on an input image, given an ExecutableNetwork\n",
    "    '''\n",
    "    # Get input image\n",
    "    image = cv2.imread(input_image)\n",
    "    # Extract the input shape\n",
    "    n, c, h, w = input_shape\n",
    "    # Preprocess it (applies for the IRs from the Pre-Trained Models lesson)\n",
    "    preprocessed_image = preprocessing(image, h, w)\n",
    "\n",
    "    # Get the input blob for the inference request\n",
    "    input_blob = next(iter(exec_net.inputs))\n",
    "\n",
    "    # Perform either synchronous or asynchronous inference\n",
    "    request_type = request_type.lower()\n",
    "    if request_type == 'a':\n",
    "        output = async_inference(exec_net, input_blob, preprocessed_image)\n",
    "    elif request_type == 's':\n",
    "        output = sync_inference(exec_net, input_blob, preprocessed_image)\n",
    "    else:\n",
    "        print(\"Unknown inference request type, should be 'A' or 'S'.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Return the exec_net for testing purposes\n",
    "    return output\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    exec_net, input_shape = load_to_IE(args.m, CPU_EXTENSION)\n",
    "    perform_inference(exec_net, args.r, args.i, input_shape)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "**test.py**\n",
    "```python\n",
    "from helpers import load_to_IE, preprocessing\n",
    "from inference import perform_inference\n",
    "\n",
    "CPU_EXTENSION = \"/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so\"\n",
    "\n",
    "MODEL_PATH = \"/home/workspace/models/\"\n",
    "\n",
    "OUTPUT_SHAPES = {\n",
    "    \"POSE\": {\"Mconv7_stage2_L1\": (1, 38, 32, 57),\n",
    "             \"Mconv7_stage2_L2\": (1, 19, 32, 57)},\n",
    "    \"TEXT\": {\"model/link_logits_/add\": (1, 16, 192, 320),\n",
    "             \"model/segm_logits/add\": (1, 2, 192, 320)},\n",
    "    \"CAR META\": {\"color\": (1, 7, 1, 1),\n",
    "                 \"type\": (1, 4, 1, 1)}\n",
    "}\n",
    "\n",
    "def pose_test():\n",
    "    counter = 0\n",
    "    model = MODEL_PATH + \"human-pose-estimation-0001.xml\"\n",
    "    image = \"images/sitting-on-car.jpg\"\n",
    "    counter += test(model, \"POSE\", image)\n",
    "\n",
    "    return counter\n",
    "\n",
    "\n",
    "def text_test():\n",
    "    counter = 0\n",
    "    model = MODEL_PATH + \"text-detection-0004.xml\"\n",
    "    image = \"images/sign.jpg\"\n",
    "    counter += test(model, \"TEXT\", image)\n",
    "\n",
    "    return counter\n",
    "\n",
    "\n",
    "def car_test():\n",
    "    counter = 0\n",
    "    model = MODEL_PATH + \"vehicle-attributes-recognition-barrier-0039.xml\"\n",
    "    image = \"images/blue-car.jpg\"\n",
    "    counter += test(model, \"CAR META\", image)\n",
    "\n",
    "    return counter\n",
    "\n",
    "\n",
    "def test(model, model_type, image):\n",
    "    # Synchronous Test\n",
    "    counter = 0\n",
    "    try:\n",
    "        # Load IE separately to check InferRequest latency\n",
    "        exec_net, input_shape = load_to_IE(model, CPU_EXTENSION)\n",
    "        result = perform_inference(exec_net, \"S\", image, input_shape)\n",
    "        output_blob = next(iter(exec_net.outputs))\n",
    "        # Check for matching output shape to expected\n",
    "        assert result[output_blob].shape == OUTPUT_SHAPES[model_type][output_blob]\n",
    "        # Check latency is > 0; i.e. a request occurred\n",
    "        assert exec_net.requests[0].latency > 0.0\n",
    "        counter += 1\n",
    "    except:\n",
    "        print(\"Synchronous Inference failed for {} Model.\".format(model_type))\n",
    "    # Asynchronous Test\n",
    "    try:\n",
    "        # Load IE separately to check InferRequest latency\n",
    "        exec_net, input_shape = load_to_IE(model, CPU_EXTENSION)\n",
    "        exec_net = perform_inference(exec_net, \"A\", image, input_shape)\n",
    "        output_blob = next(iter(exec_net.outputs))\n",
    "        # Check for matching output shape to expected\n",
    "        assert exec_net.requests[0].outputs[output_blob].shape == OUTPUT_SHAPES[model_type][output_blob]\n",
    "        # Check latency is > 0; i.e. a request occurred\n",
    "        assert exec_net.requests[0].latency > 0.0\n",
    "        counter += 1\n",
    "    except:\n",
    "        print(\"Asynchronous Inference failed for {} Model.\".format(model_type))\n",
    "\n",
    "    return counter\n",
    "\n",
    "\n",
    "def feedback(tests_passed):\n",
    "    print(\"You passed {} of 6 tests.\".format(int(tests_passed)))\n",
    "    if tests_passed == 3:\n",
    "        print(\"Congratulations!\")\n",
    "    else:\n",
    "        print(\"See above for additional feedback.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    counter = pose_test() + text_test() + car_test()\n",
    "    feedback(counter)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "**helpers.py**\n",
    "```python\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IENetwork, IECore\n",
    "\n",
    "'''\n",
    "The below functions are carried over from previous exercises.\n",
    "They are already called appropriately in inference.py.\n",
    "'''\n",
    "def load_to_IE(model_xml, cpu_extension):\n",
    "    # Load the Inference Engine API\n",
    "    plugin = IECore()\n",
    "\n",
    "    # Load IR files into their related class\n",
    "    model_bin = os.path.splitext(model_xml)[0] + \".bin\"\n",
    "    net = IENetwork(model=model_xml, weights=model_bin)\n",
    "\n",
    "    # Add a CPU extension, if applicable.\n",
    "    if cpu_extension:\n",
    "        plugin.add_extension(cpu_extension, \"CPU\")\n",
    "\n",
    "    # Get the supported layers of the network\n",
    "    supported_layers = plugin.query_network(network=net, device_name=\"CPU\")\n",
    "\n",
    "    # Check for any unsupported layers, and let the user\n",
    "    # know if anything is missing. Exit the program, if so.\n",
    "    unsupported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n",
    "    if len(unsupported_layers) != 0:\n",
    "        print(\"Unsupported layers found: {}\".format(unsupported_layers))\n",
    "        print(\"Check whether extensions are available to add to IECore.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Load the network into the Inference Engine\n",
    "    exec_net = plugin.load_network(net, \"CPU\")\n",
    "\n",
    "    # Get the input layer\n",
    "    input_blob = next(iter(net.inputs))\n",
    "\n",
    "    # Get the input shape\n",
    "    input_shape = net.inputs[input_blob].shape\n",
    "\n",
    "    return exec_net, input_shape\n",
    "\n",
    "\n",
    "def preprocessing(input_image, height, width):\n",
    "    '''\n",
    "    Given an input image, height and width:\n",
    "    - Resize to width and height\n",
    "    - Transpose the final \"channel\" dimension to be first\n",
    "    - Reshape the image to add a \"batch\" of 1 at the start \n",
    "    '''\n",
    "    image = np.copy(input_image)\n",
    "    image = cv2.resize(image, (width, height))\n",
    "    image = image.transpose((2,0,1))\n",
    "    image = image.reshape(1, 3, height, width)\n",
    "\n",
    "    return image\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='3.3'></a>3.3 Integrate IE into Edge App\n",
    "* **procedures to integrate IE into the edge app:**\n",
    "     * step 1: get an IR with model optimizer (either directly obtained or convert a pre-trained via MO)\n",
    "     * step 2: load it into app with IE\n",
    "     * step 3: add any necessary pre-processing code for input frame\n",
    "     * step 4: make inference request and preform inference\n",
    "     * step 5: handle and process output\n",
    "$$step1 \\to step2 \\to step3 \\to step4$$\n",
    "* **what the app do: convert a bounding box model to IR with MO (previously done in [exe 2.1](#2.1)); use an async request to perform inference on each video frame and extract the results from the inference request; add code to make the requests and feed back the results within the application; perform any necessary post-processing steps to get the bounding boxes**\n",
    "\n",
    "**app.py**\n",
    "```python\n",
    "import argparse\n",
    "import cv2\n",
    "from inference import Network\n",
    "\n",
    "INPUT_STREAM = \"test_video.mp4\"\n",
    "CPU_EXTENSION = \"/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so\"\n",
    "\n",
    "def get_args():\n",
    "    '''\n",
    "    Gets the arguments from the command line.\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\"Run inference on an input video\")\n",
    "    # -- Create the descriptions for the commands\n",
    "    m_desc = \"The location of the model XML file\"\n",
    "    i_desc = \"The location of the input file\"\n",
    "    d_desc = \"The device name, if not 'CPU'\"    \n",
    "    ### TODO: Add additional arguments and descriptions for:\n",
    "    ###       1) Different confidence thresholds used to draw bounding boxes\n",
    "    t_desc = \"The confidence threshold used to draw bounding boxes\"    \n",
    "    ###       2) The user choosing the color of the bounding boxes\n",
    "    c_desc = \"The color chosen for the bounding boxes, RED, GREEN, or BLUE\"\n",
    "\n",
    "    # -- Add required and optional groups\n",
    "    parser._action_groups.pop()\n",
    "    required = parser.add_argument_group('required arguments')\n",
    "    optional = parser.add_argument_group('optional arguments')\n",
    "\n",
    "    # -- Create the arguments\n",
    "    required.add_argument(\"-m\", help=m_desc, required=True)\n",
    "    optional.add_argument(\"-i\", help=i_desc, default=INPUT_STREAM)\n",
    "    optional.add_argument(\"-d\", help=d_desc, default='CPU')\n",
    "    optional.add_argument(\"-t\", help=t_desc, default=0.5)\n",
    "    optional.add_argument(\"-c\", help=c_desc, default='RED')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "def convert_color(color_str):\n",
    "    colors = {'BLUE':(255,0,0),'GREEN':(0,255,0),'RED':(0,0,255)}\n",
    "    color = colors.get(color_str)\n",
    "    if color:\n",
    "        return color\n",
    "    else:\n",
    "        return colors['RED']     # default color is red\n",
    "\n",
    "# The IR model outputs a blob with the shape: [1, 1, N, 7], where N is the number of detected bounding boxes.\n",
    "# For each detection, the description has the format: [image_id, label, conf, x_min, y_min, x_max, y_max]\n",
    "def draw_boxes(frame, result, args, width, height):\n",
    "    for box in result[0][0]:        \n",
    "        if box[2] > args.t:\n",
    "            xmin = int(box[3]*width)\n",
    "            ymin = int(box[4]*height)\n",
    "            xmax = int(box[5]*width)\n",
    "            ymax = int(box[6]*height)\n",
    "            cv2.rectangle(frame, (xmin,ymin), (xmax,ymax), args.c, 1)\n",
    "    return frame\n",
    "\n",
    "def infer_on_video(args):\n",
    "    args.c = convert_color(args.c)\n",
    "    args.t = float(args.t)\n",
    "    ### TODO: Initialize the Inference Engine\n",
    "    plugin = Network()\n",
    "    ### TODO: Load the network model into the IE\n",
    "    plugin.load_model(args.m,args.d,CPU_EXTENSION)\n",
    "    input_shape = plugin.get_input_shape()\n",
    "    # Get and open video capture\n",
    "    cap = cv2.VideoCapture(args.i)\n",
    "    cap.open(args.i)\n",
    "\n",
    "    # Grab the shape of the input \n",
    "    width = int(cap.get(3))\n",
    "    height = int(cap.get(4))\n",
    "\n",
    "    # Create a video writer for the output video\n",
    "    # The second argument should be `cv2.VideoWriter_fourcc('M','J','P','G')`\n",
    "    # on Mac, and `0x00000021` on Linux\n",
    "    out = cv2.VideoWriter('out.mp4', 0x00000021, 30, (width,height))\n",
    "    \n",
    "    # Process frames until the video ends, or process is exited\n",
    "    while cap.isOpened():\n",
    "        # Read the next frame\n",
    "        flag, frame = cap.read()\n",
    "        if not flag:\n",
    "            break\n",
    "        key_pressed = cv2.waitKey(60)\n",
    "\n",
    "        ### TODO: Pre-process the frame\n",
    "        prep_frame = cv2.resize(frame, (input_shape[3],input_shape[2])).transpose(2,0,1)        \n",
    "        prep_frame = prep_frame.reshape(1,*prep_frame.shape)\n",
    "        ### TODO: Perform inference on the frame\n",
    "        plugin.async_inference(prep_frame)\n",
    "        ### TODO: Get the output of inference\n",
    "        if plugin.wait() == 0:\n",
    "            res = plugin.extract_output()\n",
    "        ### TODO: Update the frame to include detected bounding boxes\n",
    "        frame = draw_boxes(frame, res, args, width, height)\n",
    "        # Write out the frame\n",
    "        out.write(frame)\n",
    "        # Break if escape key pressed\n",
    "        if key_pressed == 27:\n",
    "            break\n",
    "\n",
    "    # Release the out writer, capture, and destroy any OpenCV windows\n",
    "    out.release()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    infer_on_video(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "**inference.py**\n",
    "```python\n",
    "'''\n",
    "Contains code for working with the Inference Engine.\n",
    "You'll learn how to implement this code and more in\n",
    "the related lesson on the topic.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging as log\n",
    "from openvino.inference_engine import IENetwork, IECore\n",
    "\n",
    "class Network:\n",
    "    '''\n",
    "    Load and store information for working with the Inference Engine,\n",
    "    and any loaded models.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.plugin = None\n",
    "        self.network = None\n",
    "        self.input_blob = None\n",
    "        self.output_blob = None\n",
    "        self.exec_network = None\n",
    "        self.infer_request = None\n",
    "\n",
    "\n",
    "    def load_model(self, model, device=\"CPU\", cpu_extension=None):\n",
    "        '''\n",
    "        Load the model given IR files.\n",
    "        Defaults to CPU as device for use in the workspace.\n",
    "        Synchronous requests made within.\n",
    "        '''\n",
    "        model_xml = model\n",
    "        model_bin = os.path.splitext(model_xml)[0] + \".bin\"\n",
    "\n",
    "        # Initialize the plugin\n",
    "        self.plugin = IECore()\n",
    "\n",
    "        # Add a CPU extension, if applicable\n",
    "        if cpu_extension and \"CPU\" in device:\n",
    "            self.plugin.add_extension(cpu_extension, device)\n",
    "\n",
    "        # Read the IR as a IENetwork\n",
    "        self.network = IENetwork(model=model_xml, weights=model_bin)\n",
    "\n",
    "        # Load the IENetwork into the plugin\n",
    "        self.exec_network = self.plugin.load_network(self.network, device)\n",
    "\n",
    "        # Get the input layer\n",
    "        self.input_blob = next(iter(self.network.inputs))\n",
    "        self.output_blob = next(iter(self.network.outputs))\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        '''\n",
    "        Gets the input shape of the network\n",
    "        '''\n",
    "        return self.network.inputs[self.input_blob].shape\n",
    "\n",
    "\n",
    "    def async_inference(self, image):\n",
    "        '''\n",
    "        Makes an asynchronous inference request, given an input image.\n",
    "        '''\n",
    "        ### TODO: Start asynchronous inference\n",
    "        self.exec_network.start_async(request_id=0,inputs={self.input_blob:image})        \n",
    "        return\n",
    "\n",
    "\n",
    "    def wait(self):\n",
    "        '''\n",
    "        Checks the status of the inference request.\n",
    "        '''\n",
    "        ### TODO: Wait for the async request to be complete\n",
    "        status = self.exec_network.requests[0].wait()\n",
    "        return status\n",
    "\n",
    "\n",
    "    def extract_output(self):\n",
    "        '''\n",
    "        Returns a list of the results for the output layer of the network.\n",
    "        '''\n",
    "        ### TODO: Return the outputs of the network from the output_blob\n",
    "        res = self.exec_network.requests[0].outputs[self.output_blob]\n",
    "        return res\n",
    "```\n",
    "\n",
    "* **run the app will generate an output.mp4 file with bounding boxes detecting moving vehicles**\n",
    "```shell\n",
    "python app.py -m frozen_inference_graph.xml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"400\" height=\"300\" controls>\n",
       "  <source src=\"./out.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"400\" height=\"300\" controls>\n",
    "  <source src=\"./out.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 in Deploy an Edge App (Lesson 5) <a id='4'></a>\n",
    "    \n",
    "**Back to [TOC](#toc)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='4.1'></a>4.1 Handle Input Streams\n",
    "* **Task 1: Implement a function that can handle camera image, video file or webcam inputs**\n",
    "* **Task 2: Use cv2.VideoCapture() and open the capture stream, loop while the capture is open, use capture.read to return two values: boolean (false when no frame to read) and frame, read frames, and break the loop if no more frames (checked by capture.isOpened)**\n",
    "* **Task 3: Re-size the frame to 100x100**\n",
    "* **Task 4: Add Canny Edge Detection to the frame with min & max values of 100 and 200, respectively**\n",
    "* **Task 5: Save down the image or video output and close the stream and any windows at the end of the application**\n",
    "* **[OpenCV documentation and tutorial](https://docs.opencv.org/master/d9/df8/tutorial_root.html)**\n",
    "\n",
    "**app.py**\n",
    "```python\n",
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def get_args():\n",
    "    '''\n",
    "    Gets the arguments from the command line.\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\"Handle an input stream\")\n",
    "    # -- Create the descriptions for the commands\n",
    "    i_desc = \"The location of the input file\"\n",
    "\n",
    "    # -- Create the arguments\n",
    "    parser.add_argument(\"-i\", help=i_desc)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def capture_stream(args):\n",
    "    ### TODO: Handle image, video or webcam\n",
    "    flag = False\n",
    "    if args.i == \"CAM\":\n",
    "        args.i = 0\n",
    "    elif args.i.endswith('.jpg') or args.i.endswith('.bmp'):\n",
    "        flag = True\n",
    "    ### TODO: Get and open video capture\n",
    "    capture = cv2.VideoCapture(args.i)\n",
    "    capture.open(args.i)\n",
    "    if not flag:\n",
    "        # The second argument should be `cv2.VideoWriter_fourcc('M','J','P','G')`\n",
    "        # on Mac, and `0x00000021` on Linux\n",
    "        # 100x100 to match desired resizing\n",
    "        out_video = cv2.VideoWriter('out_video.mp4', 0x00000021, 20, (100,100))        \n",
    "        while capture.isOpened():            \n",
    "            pressed_key = cv2.waitKey(120)\n",
    "            if pressed_key == 27:\n",
    "                break\n",
    "            boo, frame = capture.read()\n",
    "            if not boo:\n",
    "                break\n",
    "        ### TODO: Re-size the frame to 100x100\n",
    "            frame = cv2.resize(frame, (100,100))\n",
    "        ### TODO: Add Canny Edge Detection to the frame, \n",
    "            ###       with min & max values of 100 and 200\n",
    "            ###       Make sure to use np.dstack after to make a 3-channel image\n",
    "            frame = cv2.Canny(frame,100,200)\n",
    "            frame = np.dstack((frame,frame,frame))\n",
    "            ### TODO: Write out the frame, depending on image or video\n",
    "            out_video.write(frame)            \n",
    "    else:\n",
    "        frame = cv2.imread(args.i)\n",
    "        frame = cv2.resize(frame,(100,100))\n",
    "        frame = cv2.Canny(frame,(100,200))\n",
    "        frame = np.dstack((frame,frame,frame))\n",
    "        cv2.imwrite('out_image.jpg',frame)\n",
    "    ### TODO: Close the stream and any windows at the end of the application\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    capture_stream(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "* **run the app will generate an output_video.mp4 file upon canny edge detection**\n",
    "```shell\n",
    "python app.py -i test_video.mp4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"400\" height=\"300\" controls>\n",
       "  <source src=\"./out_video.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"400\" height=\"300\" controls>\n",
    "  <source src=\"./out_video.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='4.2'></a>4.2 Process Model Output\n",
    "* **Suppose you have a video showing two combinations, one for cat and dog#1 who are friends, the other for cat and dog#2 who don't get along, your goal is to print a warning message to the terminal when cat and dog#2 seen since they don't get along**\n",
    "* **```model.xml```: object detection model that can identify different breeds, will return three classes, one for one or less pets on screen, one for the bad combination of the cat and dog#2, and one for the fine combination of the cat and dog#1.**\n",
    "* **```python app.py -m model.xml -i pets.mp4```: run app will print out a warning message \"Warning: Break up!\" at 3.8 s, 10.8 s, 17.8 s.**\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "import cv2\n",
    "from inference import Network\n",
    "\n",
    "INPUT_STREAM = \"pets.mp4\"\n",
    "CPU_EXTENSION = \"/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so\"\n",
    "\n",
    "def get_args():\n",
    "    '''\n",
    "    Gets the arguments from the command line.\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\"Run inference on an input video\")\n",
    "    # -- Create the descriptions for the commands\n",
    "    m_desc = \"The location of the model XML file\"\n",
    "    i_desc = \"The location of the input file\"\n",
    "    d_desc = \"The device name, if not 'CPU'\"\n",
    "\n",
    "    # -- Add required and optional groups\n",
    "    parser._action_groups.pop()\n",
    "    required = parser.add_argument_group('required arguments')\n",
    "    optional = parser.add_argument_group('optional arguments')\n",
    "\n",
    "    # -- Create the arguments\n",
    "    required.add_argument(\"-m\", help=m_desc, required=True)\n",
    "    optional.add_argument(\"-i\", help=i_desc, default=INPUT_STREAM)\n",
    "    optional.add_argument(\"-d\", help=d_desc, default='CPU')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def infer_on_video(args):\n",
    "    # Initialize the Inference Engine\n",
    "    plugin = Network()\n",
    "\n",
    "    # Load the network model into the IE\n",
    "    plugin.load_model(args.m, args.d, CPU_EXTENSION)\n",
    "    net_input_shape = plugin.get_input_shape()\n",
    "\n",
    "    # Get and open video capture\n",
    "    cap = cv2.VideoCapture(args.i)\n",
    "    cap.open(args.i)\n",
    "\n",
    "    # Process frames until the video ends, or process is exited\n",
    "    count = 0 \n",
    "    pet_flag = False\n",
    "    while cap.isOpened():\n",
    "        # Read the next frame\n",
    "        flag, frame = cap.read()\n",
    "        if not flag:\n",
    "            break\n",
    "        key_pressed = cv2.waitKey(60)\n",
    "        count += 1\n",
    "        # Pre-process the frame\n",
    "        p_frame = cv2.resize(frame, (net_input_shape[3], net_input_shape[2]))\n",
    "        p_frame = p_frame.transpose((2,0,1))\n",
    "        p_frame = p_frame.reshape(1, *p_frame.shape)\n",
    "\n",
    "        # Perform inference on the frame\n",
    "        plugin.async_inference(p_frame)\n",
    "\n",
    "        # Get the output of inference\n",
    "        if plugin.wait() == 0:\n",
    "            result = plugin.extract_output()\n",
    "            ### TODO: Process the output, second class is the combination of cat and dog#2\n",
    "            if result[0][1] == 1 and not pet_flag:\n",
    "                print('Warning: Break up!')\n",
    "                # 30 frames per second\n",
    "                print('Incident at {} seconds'.format(count/30))\n",
    "                pet_flag = True\n",
    "            elif result[0][1] != 1:\n",
    "                pet_flag = False\n",
    "\n",
    "        # Break if escape key pressed\n",
    "        if key_pressed == 27:\n",
    "            break\n",
    "\n",
    "    # Release the capture and destroy any OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    infer_on_video(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"400\" height=\"300\" controls>\n",
       "  <source src=\"./pets.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"400\" height=\"300\" controls>\n",
    "  <source src=\"./pets.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='4.3'></a>4.3 Server Communications and Run App on Web\n",
    "* **Task 1: Add any code for MQTT to the project so that the node server receives the calculated stats**\n",
    "* **Taks 2: Send the output frame (not the input image, but the processed output) to the ffserver**\n",
    "* **Task 3: Get the MQTT broker and UI installed and running. ```cd webservice/server``` --> ```npm install``` --> When complete, ```cd ../ui``` --> And again, ```npm install```. Then start 4 terminals for each.**\n",
    "    * Terminal 1: Get the MQTT broker installed and running. ```cd webservice/server/node-server``` --> ```node ./server.js``` --> You should see a message that ```Mosca server started```.\n",
    "    * Terminal 2: Get the UI Node Server running. ```cd webservice/ui``` --> ```npm run dev``` --> After a few seconds, you should see ```webpack: Compiled successfully```.\n",
    "    * Terminal 3: Start the ffserver. ```sudo ffserver -f ./ffmpeg/server.conf``` ffmpege is a folder containing configuration file to set the port & IP address of server, ports to receive video from, and framerate.\n",
    "    * Terminal 4: Start the actual application. First, you need to source the environment for OpenVINO in the new terminal: ```source /opt/intel/openvino/bin/setupvars.sh -pyver 3.5```\n",
    "* **[```paho-mqtt``` library](https://pypi.org/project/paho-mqtt/): a MQTT library in Python, used to connect to client with IP address and port of broker (server).**\n",
    "* **```ffserver``` feature of [FFmpeg](https://www.ffmpeg.org/): a software/library for handling video and audio streams, have an intermediate FFmpeg server that video frames are sent to then send onto Node server for a webpage. [Flask](https://www.pyimagesearch.com/2019/09/02/opencv-stream-video-to-web-browser-html-page/) library in Python can also be used for this purpose.**\n",
    "* **[```Node.js``` server](https://nodejs.org/en/about/): an open-source server environment, allows JavaScript to be run outside of a browser and get dynamic contents for a user and display in browser.**\n",
    "\n",
    "**app.py**\n",
    "```python\n",
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "import socket\n",
    "import json\n",
    "from random import randint\n",
    "from inference import Network\n",
    "### TODO: Import any libraries for MQTT and FFmpeg\n",
    "import sys\n",
    "import paho.mqtt.client as mqtt\n",
    "\n",
    "INPUT_STREAM = \"test_video.mp4\"\n",
    "CPU_EXTENSION = \"/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so\"\n",
    "ADAS_MODEL = \"/home/workspace/models/semantic-segmentation-adas-0001.xml\"\n",
    "\n",
    "\n",
    "CLASSES = ['road', 'sidewalk', 'building', 'wall', 'fence', 'pole', \n",
    "'traffic_light', 'traffic_sign', 'vegetation', 'terrain', 'sky', 'person',\n",
    "'rider', 'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle', 'ego-vehicle']\n",
    "\n",
    "# MQTT server environment variables\n",
    "HOSTNAME = socket.gethostname()\n",
    "IPADDRESS = socket.gethostbyname(HOSTNAME)\n",
    "MQTT_HOST = IPADDRESS\n",
    "MQTT_PORT = 3001 ### TODO: Set the Port for MQTT, the MQTT port to use is 3001\n",
    "MQTT_KEEPALIVE_INTERVAL = 60\n",
    "\n",
    "def get_args():\n",
    "    '''\n",
    "    Gets the arguments from the command line.\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\"Run inference on an input video\")\n",
    "    # -- Create the descriptions for the commands\n",
    "    i_desc = \"The location of the input file\"\n",
    "    d_desc = \"The device name, if not 'CPU'\"\n",
    "\n",
    "    # -- Create the arguments\n",
    "    parser.add_argument(\"-i\", help=i_desc, default=INPUT_STREAM)\n",
    "    parser.add_argument(\"-d\", help=d_desc, default='CPU')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def draw_masks(result, width, height):\n",
    "    '''\n",
    "    Draw semantic mask classes onto the frame.\n",
    "    '''\n",
    "    # Create a mask with color by class\n",
    "    classes = cv2.resize(result[0].transpose((1,2,0)), (width,height), \n",
    "        interpolation=cv2.INTER_NEAREST)\n",
    "    unique_classes = np.unique(classes)\n",
    "    out_mask = classes * (255/20)\n",
    "    \n",
    "    # Stack the mask so FFmpeg understands it\n",
    "    out_mask = np.dstack((out_mask, out_mask, out_mask))\n",
    "    out_mask = np.uint8(out_mask)\n",
    "\n",
    "    return out_mask, unique_classes\n",
    "\n",
    "\n",
    "def get_class_names(class_nums):\n",
    "    class_names= []\n",
    "    for i in class_nums:\n",
    "        class_names.append(CLASSES[int(i)])\n",
    "    return class_names\n",
    "\n",
    "\n",
    "def infer_on_video(args, model):\n",
    "    ### TODO: Connect to the MQTT server\n",
    "    mqttc = mqtt.Client()\n",
    "    mqttc.connect(MQTT_HOST, MQTT_PORT, MQTT_KEEPALIVE_INTERVAL)\n",
    "    \n",
    "    # Initialize the Inference Engine\n",
    "    plugin = Network()\n",
    "\n",
    "    # Load the network model into the IE\n",
    "    plugin.load_model(model, args.d, CPU_EXTENSION)\n",
    "    net_input_shape = plugin.get_input_shape()\n",
    "\n",
    "    # Get and open video capture\n",
    "    cap = cv2.VideoCapture(args.i)\n",
    "    cap.open(args.i)\n",
    "\n",
    "    # Grab the shape of the input \n",
    "    width = int(cap.get(3))\n",
    "    height = int(cap.get(4))\n",
    "\n",
    "    # Process frames until the video ends, or process is exited\n",
    "    while cap.isOpened():\n",
    "        # Read the next frame\n",
    "        flag, frame = cap.read()\n",
    "        if not flag:\n",
    "            break\n",
    "        key_pressed = cv2.waitKey(60)\n",
    "\n",
    "        # Pre-process the frame\n",
    "        p_frame = cv2.resize(frame, (net_input_shape[3], net_input_shape[2]))\n",
    "        p_frame = p_frame.transpose((2,0,1))\n",
    "        p_frame = p_frame.reshape(1, *p_frame.shape)\n",
    "\n",
    "        # Perform inference on the frame\n",
    "        plugin.async_inference(p_frame)\n",
    "\n",
    "        # Get the output of inference\n",
    "        if plugin.wait() == 0:\n",
    "            result = plugin.extract_output()\n",
    "            # Draw the output mask onto the input\n",
    "            out_frame, classes = draw_masks(result, width, height)\n",
    "            class_names = get_class_names(classes)\n",
    "            speed = randint(50,70)\n",
    "            \n",
    "            ### TODO: Send the class names and speed to the MQTT server\n",
    "            ### The topics that the UI Node Server is listening to are \"class\" and \"speedometer\"\n",
    "            ### Hint: The UI web server will check for a \"class\" and\n",
    "            ### \"speedometer\" topic. Additionally, it expects \"class_names\"\n",
    "            ### and \"speed\" as the json keys of the data, respectively.\n",
    "            mqttc.publish('class', json.dumps({'class_names': class_names}))\n",
    "            mqttc.publish('speedometer', json.dumps({'speed': speed}))\n",
    "\n",
    "        ### TODO: Send frame to the ffmpeg server\n",
    "        sys.stdout.buffer.write(out_frame)\n",
    "        sys.stdout.flush\n",
    "\n",
    "        # Break if escape key pressed\n",
    "        if key_pressed == 27:\n",
    "            break\n",
    "\n",
    "    # Release the capture and destroy any OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ### TODO: Disconnect from MQTT\n",
    "    mqttc.disconnect()\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    model = ADAS_MODEL\n",
    "    infer_on_video(args, model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "* **Run ```app.py``` in Terminal 4, and can see MQTT broker server noting information getting published**\n",
    "    * Pipe the output frames into FFmpeg: ```python app.py | ffmpeg {args}```, arguments like ```-f``` (format), ```-pixel_format```, ```-video_size```, ```-framerate```.\n",
    "    * The video is running slowly because semantic segmentation is pretty resoursive and intensive\n",
    "\n",
    "```bash\n",
    "python app.py | ffmpeg -video_size 1280x720  -f rawvideo -pixel_format bgr24 -framerate 24 -i - http://0.0.0.0:3004/fac.ffm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"400\" height=\"300\" controls>\n",
       "  <source src=\"./Vehicle_Edge_Application.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"400\" height=\"300\" controls>\n",
    "  <source src=\"./Vehicle_Edge_Application.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
